{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation with Keras",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvcXXRjEYyuefNXU2D7l95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DragosBrex/portofolio/blob/main/Machine_Translation_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8IImNXQXxlT"
      },
      "outputs": [],
      "source": [
        "#Importam librariile necesare\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import helper\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conectare Colab la Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaCyKVz9X2mZ",
        "outputId": "55ae3911-1de9-40ed-a696-5f2550d78d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Citirea dataset-ului din Google Drive\n",
        "!cp '/content/drive/My Drive/Diacritice Model - Folder/data.csv' ."
      ],
      "metadata": {
        "id": "ouHimlorYFef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Afisarea fisierelor citite\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMXBQX1eYG4F",
        "outputId": "83c4b5a8-fc7d-4b2e-8319-44b7fb548152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.csv  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Citirea datelor din dataset intr-o variabila numita culmea \"data\"\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "\"data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2mLLW77cYHvl",
        "outputId": "1f55b55e-d440-4bb7-8940-1f2430fa2a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definim o variabila care sa contina semnele de punctuatie si caracterele speciale\n",
        "strip_chars = string.punctuation \n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "strip_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OXSuXyaJYkaB",
        "outputId": "a251698f-5900-4906-fdab-721881d87bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simplificam datele, stergand semnele de punctuatie si caracterele speciale. Apoi transformam toate literele in litere mici\n",
        "\n",
        "for line in range(len(data[\"Input\"])):\n",
        "  for caracter in data[\"Input\"][line]:\n",
        "    if strip_chars.find(caracter) >= 0:\n",
        "      data[\"Input\"][line] = data[\"Input\"][line].replace(caracter, \"\")\n",
        "      data[\"Output\"][line] = data[\"Output\"][line].replace(caracter, \"\")\n",
        "\n",
        "for line in range(len(data[\"Input\"])):\n",
        "  data[\"Input\"][line] = data[\"Input\"][line].lower()\n",
        "  data[\"Output\"][line] = data[\"Output\"][line].lower()"
      ],
      "metadata": {
        "id": "wSUKEaJab3KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Amestecam datele si le impartim in date pentru test \n",
        "data = shuffle(data) \n",
        "\n",
        "train_size = int(len(data)*0.8)\n",
        "test_size = int(len(data)*0.2)\n",
        "\n",
        "\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size + 1 : ]\n",
        "\n",
        "print(\"Train size: %d\" % train_size)\n",
        "print(\"Test size: %d\" % test_size)\n",
        "\n",
        "print(\"Total size: %d\" % (train_size+test_size))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BZLG_Y90YQB",
        "outputId": "76d1667c-6ae1-489a-b894-5c160fb3728f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 5316\n",
            "Test size: 1329\n",
            "Total size: 6645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definim o functie ce va transorma cuvintele noastre in id-uri numerice\n",
        "def tokenize(x):\n",
        "    \n",
        "    tokenizer = Tokenizer(char_level=False) # char_level trebuie neaparat setat pe valoarea \"False\"\n",
        "    tokenizer.fit_on_texts(x)\n",
        "\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "metadata": {
        "id": "3JST9By-AR2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definim o functie ce va egala lungimea textelor ce vor fi comparate, deoarece... si aici explica Dragos\n",
        "def pad(x, length=None):\n",
        "    \n",
        "    if length is None:\n",
        "        \n",
        "        length = max([len(sentence) for sentence in x])\n",
        "        print(length)\n",
        "        \n",
        "    return pad_sequences(x, maxlen=length, padding='post')\n"
      ],
      "metadata": {
        "id": "-nLmA4MFTZ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definim o functie care aplica cele doua functii de mai sus datelor noastre\n",
        "def preprocess(x, y):\n",
        "\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "\n",
        "text_fd_train, text_cd_train, fd_romanian_tokenizer, cd_romanian_tokenizer = preprocess(train_data[\"Input\"], train_data[\"Output\"])\n",
        "    \n",
        "max_fd_length = text_fd_train.shape[1] # _fd_ = \"fara diacritice\"\n",
        "max_cd_length = text_cd_train.shape[1]   # _cd_ = \"cu diacritice\"\n",
        "\n",
        "fd_size = len(fd_romanian_tokenizer.word_index)\n",
        "cd_size = len(cd_romanian_tokenizer.word_index)\n",
        "\n",
        "print(\"Cea mai lunga secventa de text fara diacritice are %d\" % max_fd_length + \" de caractere\")\n",
        "print(\"Cea mai lunga secventa de text cu diacritice are %d\" % max_cd_length + \" de caractere\")\n",
        "print(\"Dimensiunea vocabularului:\", cd_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtlU0vpvTcPF",
        "outputId": "e52746ce-f3ce-4af9-dc96-cc98a8ad9ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "21\n",
            "Cea mai lunga secventa de text fara diacritice are 21 de caractere\n",
            "Cea mai lunga secventa de text cu diacritice are 21 de caractere\n",
            "Dimensiunea vocabularului: 10958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In final cream modelul si il antrenam\n",
        "def the_model(input_shape, output_sequence_length, input_size, output_size):\n",
        "\n",
        "    learning_rate = 1e-3 \n",
        "\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    \n",
        "    rnn = GRU(64, return_sequences=True)(input_seq)\n",
        "    \n",
        "    logits = TimeDistributed(Dense(output_size))(rnn)\n",
        "\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    \n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "fd_train_data = pad(text_fd_train, max_cd_length)\n",
        "fd_train_data = fd_train_data.reshape(text_cd_train.shape)\n",
        "\n",
        "cd_train_data = text_cd_train\n",
        "\n",
        "#Antrenarea modelului\n",
        "model = the_model(\n",
        "    fd_train_data.shape,\n",
        "    max_cd_length,\n",
        "    fd_size,\n",
        "    cd_size)\n",
        "\n",
        "model.fit(fd_train_data, cd_train_data, batch_size=1024, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "ptoytJfaWko_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d233723-bf73-4c80-d669-48151af8b621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 4s 238ms/step - loss: 9.2422 - accuracy: 0.2868 - val_loss: nan - val_accuracy: 0.4314\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 9.0676 - accuracy: 0.4247 - val_loss: nan - val_accuracy: 0.4313\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 8.8385 - accuracy: 0.4247 - val_loss: nan - val_accuracy: 0.4313\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 8.5092 - accuracy: 0.4247 - val_loss: nan - val_accuracy: 0.4313\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 8.0285 - accuracy: 0.4247 - val_loss: nan - val_accuracy: 0.4313\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 7.4751 - accuracy: 0.4246 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 6.9843 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 6.5548 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 6.1760 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 5.8414 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 5.5528 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 5.3048 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 5.0885 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 4.9013 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 4.7510 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.6427 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 4.5734 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.5289 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.4971 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 4.4746 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 4.4509 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 4.4257 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 4.4020 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 4.3807 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.3613 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.3408 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 4.3227 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.3062 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 4.2912 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.2774 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 4.2644 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.2519 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 4.2399 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.2281 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.2168 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.2056 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.1948 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.1843 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.1742 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.1642 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 4.1544 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.1448 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.1354 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.1262 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.1172 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 4.1085 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 4.0999 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 4.0914 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.0829 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 4.0747 - accuracy: 0.4245 - val_loss: nan - val_accuracy: 0.4310\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0525038610>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definim o functie ce transforma id-urile numerice inapoi in cuvinte\n",
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "DCErltKTfAhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_fd_test, text_cd_test, fd_romanian_tokenizer, cd_romanian_tokenizer = preprocess(test_data[\"Input\"], test_data[\"Output\"])\n",
        "\n",
        "max_fd_length = text_fd_test.shape[1] # _fd_ = \"fara diacritice\"\n",
        "max_cd_length = text_cd_test.shape[1]   # _cd_ = \"cu diacritice\"\n",
        "\n",
        "fd_size = len(fd_romanian_tokenizer.word_index)\n",
        "cd_size = len(cd_romanian_tokenizer.word_index)\n",
        "\n",
        "print(\"Cea mai lunga secventa de text fara diacritice are %d\" % max_fd_length + \" de caractere\")\n",
        "print(\"Cea mai lunga secventa de text cu diacritice are %d\" % max_cd_length + \" de caractere\")\n",
        "print(\"Dimensiunea vocabularului:\", cd_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFHjZ-kXmvug",
        "outputId": "bf905f5c-9f46-4678-a7a0-8205785fd9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "20\n",
            "Cea mai lunga secventa de text fara diacritice are 20 de caractere\n",
            "Cea mai lunga secventa de text cu diacritice are 20 de caractere\n",
            "Dimensiunea vocabularului: 4580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd_test_data = pad(text_fd_test, max_cd_length)\n",
        "fd_test_data = fd_train_data.reshape(text_cd_train.shape)"
      ],
      "metadata": {
        "id": "xNHHOtOs2g4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd_test_data = text_cd_test"
      ],
      "metadata": {
        "id": "9sz6bcVl29aW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}